[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Daniela Delgado\nMachine Learning: Course Blog Posts\nMiddlebury College ’24"
  },
  {
    "objectID": "WarmUp1.html",
    "href": "WarmUp1.html",
    "title": "Daniela's CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\n#show table\ndf\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nPAL0910\n120\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A2\nNo\n12/1/09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nPAL0910\n121\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A1\nYes\n11/22/09\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n8.41151\n-26.13832\nNaN\n\n\n341\nPAL0910\n122\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A2\nYes\n11/22/09\n50.4\n15.7\n222.0\n5750.0\nMALE\n8.30166\n-26.04117\nNaN\n\n\n342\nPAL0910\n123\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A1\nYes\n11/22/09\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n8.24246\n-26.11969\nNaN\n\n\n343\nPAL0910\n124\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A2\nYes\n11/22/09\n49.9\n16.1\n213.0\n5400.0\nMALE\n8.36390\n-26.15531\nNaN\n\n\n\n\n344 rows × 17 columns\n\n\n\n\n#Warm-Up part A\n\n#group dataset based on sex and species to find mean mass\n\nmeanMassTable = df.groupby(['Species', 'Sex']).aggregate({'Body Mass (g)' : 'mean'}) #using dictionary to apply the mean function on the data\nprint(meanMassTable)\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066\n\n\n\n#Warm-up part B\n\n#Scatterplot of culmen length against culmen depth (bill dimensions), with the color of each point corresponding to the penguin species\n\n#creating scatterplot \nsns.scatterplot(data = df, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Species')\n\n#labeling plot\nplt.title('Culmen Length vs. Culmen Depth Based on Penguin Species')\nplt.show()"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/classifying-penguins/index.html",
    "href": "posts/classifying-penguins/index.html",
    "title": "Classifying Palmer Penguins Post",
    "section": "",
    "text": "Abstract\nIn this blog post, we will predict the species of a penguin based on its measurements and aim to achieve 100% testing accuracy with my select choice in model. First, I will do data preparation where I will clean the dataset by removing unnecessary columns, labels, and, of course, the species column. Then, by looking at this new dataset, I will make graphs using seaborn based on the information given to see if I can spot any patterns that can help identify a penguin’s species. In my graphs, I look at flipper length in relation to body mass, culmen depth and length in relation to the penguin’s sex, and the correlation between the penguin’s culmen depth and lengh with the flipper length. Then I made a summary table where I found the mean and the standard deviation of the culmen length and depth, the flipper length, and the body mass of the penguins on each island based on their sex to see the areas where penguins had a closer correlation to. Then, using LogisticRegression and DecisionTreeClassifier, we will find the best mean score and its corresponding column out of every set of 3 features- two quantitative and one qualitative features. We will also test this on our testing data. Afterwards, we will plot a graph panel of decision regions for the classifiers where we will see the DTC predictions for the penguin species based on the best column we got from finding the best mean with the DTC. Finally, we will see the errors our prediction model made in comparison to the actual data by looking at a color-coded confusion matrix.\nAs the results will show, the DecisionTreeClassifier model worked better than the LogisticRegression model yet, in the test data, LogisticRegression returned a better output with 100% while DTC returned about 98.5% accuracy. This can be due to the depth limit placed on the DTC in comparison to no limit set on the LR model. Additionally, the confusion matrix showed the DTC made a very small margin error with one penguin being mislabeled. The prediction DTC classified this penguin as Chinstrap when it was actually a Gentoo penguin. Overall, the prediction model was accurate.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"]) #fit encoder on 'Species'\n\ndef prepare_data(df):\n  #removing columns that are not needed\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  #removing when sex = .\n  df = df[df[\"Sex\"] != \".\"]\n  #removing N/A labels\n  df = df.dropna()\n  #print(df)\n  #turns the labels in 'Species' to a number\n  y = le.transform(df[\"Species\"])\n  #print(y)\n  #removing 'Species' col bc now held by y\n  df = df.drop([\"Species\"], axis = 1)\n  #converted into “one-hot encoded” 0-1 columns\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nPart 1: Exploring the data:\nHere, I constructed three interesting displayed figures and one summary table that will help draw conclusions about what features I can use for the model (DTC model).\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(data = X_train, x = 'Flipper Length (mm)', y = 'Body Mass (g)', hue = 'Flipper Length (mm)')\n\nplt.title('Scatterplot of Flipper Length in Relation to Body Mass')\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we see the correlation between the flipper length and the body mass of penguins as the data is grouped by the flipper length. In the graph, the correlation between both seems to be positive as the longer the flipper length, the higher the body mass. This can help define a species as maybe some penguin species are smaller, which would allow us to know their body mass is lighter, thus their flipper length is smaller as well. This can be used in my model as it can use the weigths/sizes of penguins to possibibly help distinguish the species, along with the help of looking at other features to understand the outlier points in this set.\n\nsns.scatterplot(data = X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Sex_FEMALE')\nplt.title('Culmen Length and Depth Based on Gender')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we can see the culmen depth and length in relation to the penguin’s sex. The female culmens tend to be on the lower half of the graph. There also seems to be three different clusters: one on the middle left averaging on the point (40,18), another one starting at (45,16) and expanding upwards right to (50,20), and the final one being on the mid-lower part of the graph at about (45,14) to (50,16). In these three clusters, the female’s culmen dimensions are smaller than the men’s. This can tell us that the species of penguins all have different culmen lengths and depths, and they vary by sex. Thus, the penguin species can be distinguishable by their culmen sizes. This information can be useful in modeling as we can see if the culmen length and culmen depth compared to the gender play a role in distinguishing the species.\n\nsns.scatterplot(data = X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Flipper Length (mm)')\nplt.title('Correlation Between Culmen Length and Depth, and Flipper Length')\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we can see the correlation between the flipper length and the culmen depth/length. We can see how the deeper the culmen depth goes, the smaller the flipper and culmen lengths are. If the penguin has a medium to high range in culmen length with a lower depth, then the flipper lenght is longer. However, if both the culmen length and depth are high, then the flipper length is in about the middle size. This information can be used in modeling as we can see if the species of the penguins are identifiable based on the different correlations between the culmen dimensions and the flipper length.\n\nmeanTable = X_train.groupby(['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Sex_MALE', 'Sex_FEMALE']).aggregate(\n    {'Culmen Length (mm)' : ['mean', 'std'], 'Culmen Depth (mm)' : ['mean', 'std'], 'Flipper Length (mm)' : ['mean', 'std'], 'Body Mass (g)' : ['mean', 'std']})\nmeanTable\n\n\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nSex_MALE\nSex_FEMALE\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\nFalse\nTrue\nFalse\nTrue\n37.537500\n2.111200\n17.475000\n0.863713\n188.000000\n4.661902\n3309.375000\n251.143219\n\n\nTrue\nFalse\n40.961111\n3.273268\n19.300000\n1.058856\n196.000000\n5.455704\n4100.000000\n370.611577\n\n\nTrue\nFalse\nFalse\nTrue\n43.041176\n5.368843\n17.629412\n0.767670\n190.784314\n5.923896\n3471.078431\n267.225212\n\n\nTrue\nFalse\n46.176087\n5.930081\n19.058696\n0.971271\n196.782609\n7.828475\n4027.173913\n328.541647\n\n\nTrue\nFalse\nFalse\nFalse\nTrue\n43.423077\n4.061703\n15.093846\n1.727795\n206.507692\n11.936189\n4356.538462\n655.569244\n\n\nTrue\nFalse\n46.595000\n4.511583\n16.696667\n1.738007\n212.366667\n15.248192\n5076.250000\n721.116498\n\n\n\n\n\n\n\nIn this table, we can see the mean and standard deviation values for the culmen length and depth, the flipper length, and the body mass of the penguins on each island based on their sex. This table can be helpful as we can see the how the dimensions differ based on the island the penguins are on and their sex. These can be a telling factor on the kind of species they are by seeing the grouping they fall into and if the standard deviation is too big, then we know there are more outliers that are not close to the mean. Thus, this shows that there might not be a correlatioin with that said data.\nPart 2: Choosing Features\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score #use this isntead of for-loop\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\n\n#warnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category = FutureWarning)\n\n\nscore_counter = 0\n# these are not actually all the columns: you'll\n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Egg Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    print(cols)\n    # training LR model and scoring it\n    LR = LogisticRegression()\n    #using cross validation on LR to avoid overfitting\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5) #training on remaining 80% of data\n\n    if cv_scores_LR.mean() &gt; score_counter:\n      #updating the best score and columns of that score\n      score_counter = cv_scores_LR.mean()\n      col_best = cols\n\nprint('Best Score: ', score_counter)\nprint('Best Three Columns: ', col_best)\n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Flipper Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Flipper Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Flipper Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Length (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\n['Flipper Length (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\nBest Score:  0.9961538461538462\nBest Three Columns:  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n# LR with the selected better columns from the code above\ncols = col_best\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\nNow to test LogisticRegression:\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nUsing DecisionTreeClassifier instead of Logistic Regression, here is another model choice:\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score #use this isntead of for-loop\nfrom sklearn.tree import DecisionTreeClassifier\n\nscore_counter = 0\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Egg Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols:\n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols =  list(pair) + qual_cols\n    print(cols)\n\n    for i in range(1,20):\n      #setting the max depth to be between 1-19 and iterating through each one\n      DTC = DecisionTreeClassifier(max_depth=i)\n      #using cross validation on DTC to avoid overfitting\n      cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n      #updating the best score and columns of that score\n      if cv_scores_DTC.mean() &gt; score_counter:\n        score_counter = cv_scores_DTC.mean()\n        col_best = cols\n    \nprint('Best Score: ', score_counter)\nprint('Best Three Columns: ', col_best)\nprint('At depth: ', i)\n    \n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Flipper Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Flipper Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Flipper Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Length (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\n['Flipper Length (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\nBest Score:  0.9765460030165913\nBest Three Columns:  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\nAt depth:  19\n\n\n\n#Feeding the best cols\ncols = col_best\n\nDTC = DecisionTreeClassifier(max_depth=i)\nDTC.fit(X_train[cols], y_train)\nDTC.score(X_train[cols], y_train)\n\n1.0\n\n\nNow to test the data we do:\n\n#download the test data set \ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n#prepping data\nX_test, y_test = prepare_data(test)\nDTC.score(X_test[cols], y_test)\n\n0.9852941176470589\n\n\nNow we are going to plot a panel of decision regions for the classifiers:\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    #sets first two columns, which are quantitative, of X_train[cols]\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    #says remaining columns are one-hot qualitative columns\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(DTC, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nFinally, we will show a confusion matrix on the DTC model, which will be evaluated on the test set.\n\nfrom sklearn.metrics import confusion_matrix\n\nactual = y_test\npredict = DTC.predict(X_test[cols])\n\nconf_matrix = confusion_matrix(actual, predict)\n\nprint(conf_matrix)\n\n[[31  0  0]\n [ 0 10  1]\n [ 0  0 26]]\n\n\nIn this confusion matrix, we see the error is low, but it is seen with the Gentoo and Chinstrap penguins. The data predicted one penguin to be Chinstrap when it was actually a Gentoo penguin. With this information, I do not believe there was an error greater than others as that seems to be the only error.\nDiscussion\nIn this blog post, I found that using a DecisionTreeClassifier model worked better than the LogisticRegression model as seen by the best score prints, yet, in the test data, LogisticRegression returned a better output with 100% while DTC returned about 98.5% accuracy. I believe this is because LR does not have a limit on the iterations while DTC has a set depth, which can limit it from getting to 100% accuracy. Furthermore, the graphs explored in the beginning did help classify the data to guess the correct species, specifically the penguin’s culmen dimensions correlating to its sex as that ended up being the best column outcome for the DTC! The three clusters I saw in that graph also proved to help identify the species as seen in the plotted graph panel of decision regions for the species classifiers. Lastly, as we can see from the confusion matrix, we can conclude the DTC prediction model is accurate.\nThrough the coding process, I learned and felt more comfortable in working with seaborn and the different graphs available to make since, in part one, I originally created about seven different graphs from pairplots, to boxplots, to bar graphs before deciding on my final three. Additionally, I learned what the DecisionTreeClassifies does and the different methods it has to help with prediction and other needs. One more thing I learned was what a confusion matrix was. I had to do research on it as I had no idea what the purpose of it was nor how to use the data I had to create it. However, with the help of Geeks4Geeks and the scikitlearn website on confusion matrices, I understood the purpose and what data to look at."
  },
  {
    "objectID": "WarmUp2.html",
    "href": "WarmUp2.html",
    "title": "Daniela's CSCI 0451 Blog",
    "section": "",
    "text": "Graphing Decision Boundaries\nPart A:\nWe use the dot product equation \\(w_i x_i + w_j x_j = b\\) and substitute the values for w and b: \\[(1) * x_i + -\\dfrac 1 2 * x_j = \\dfrac 1 2\\] We make the equation into \\(y = mx + b\\) to plot it: \\[ x_j = 2 * x_i - 1\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#making range for x\nx_vals = np.linspace(-5, 10, num = 10)\n\n#solving for x2\nx2_vals = 2 * x_vals - 1\n\nplt.plot(x_vals, x2_vals, color = 'pink')\n\nplt.xlabel = ('x1')\nplt.ylabel = ('x2')\nplt.title = (\"Graph of 1(x1) + (-1/2)x2 = 1/2\")\nplt.grid (True, linestyle = '--')\nplt.show()\n\n\n\n\n\n\n\n\n\nPart B:\nWrite a quick Python function called linear_classify(x, w, b). w and x should both be 1d numpy arrays of the same length, and b should be a scalar. The function np.dot(x, w) will compute the inner product of x and w. Argument b should be a scalar number. Your function should return 0 if dot product of w and x &lt; b and 1 if the dot product of w and x &gt;= b.\n\nimport numpy as np\ndef linear_classify(x, w, b):\n    if  np.dot(w, x) &lt; b: \n        return 0\n    elif (w@x) &gt;= b: \n        return 1\n    \n#testing\n    #should return 1 since dot prod &gt; b\nw = np.array([1, 2, 3, 4])\nx = np.array([2, 6, -1, 0])\nb = 3\n    #should return 0 since dot prod &lt; b\nw1 = np.array([1, 2, 3, 4])\nx1 = np.array([2, 6, -1, 0])\nb1 = 30\n\nw2 = np.array([2, -10, -5])\nx2 = np.array([2, -1, 0])\nb2 = 10\n\ntesting = linear_classify(w, x, b)\ntesting2 = linear_classify(w1, x1, b1)\ntesting3 = linear_classify(w2, x2, b2)\n\nprint('Test 1: ', testing)\nprint('Test 2: ', testing2)\nprint('Test 2: ', testing3)\n\n\nTest 1:  1\nTest 2:  0\nTest 2:  1\n\n\nPart C:\nMake a sketch of the curve in the nonnegative quadrant defined by the equation given.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#making range for x\nx_vals = np.linspace(0, 10)\nx1_val = np.sqrt(1 - 1/4 * (x_vals**2))\n\nplt.plot(x_vals, x1_val, color = 'pink')\nplt.xlabel = ('x')\nplt.ylabel = ('x1')\nplt.title = ('Curve in Nonnegative Quadrant')\nplt.grid (True, linestyle = '--')\nplt.show()\n\n/var/folders/33/b3h8vrcd0ss7_3q4gmmnscl80000gn/T/ipykernel_2446/4100891223.py:6: RuntimeWarning: invalid value encountered in sqrt\n  x1_val = np.sqrt(1 - 1/4 * (x_vals**2))"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "Figure 1: An image of the Earth!\nFigure 1 is an image of the Earth.\nFigure 2 is a comic by Randall Munroe.\n\\[ F = ma \\]\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} }\ne^{-\\frac{1}{2}\n\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\n}\n\\]\nAligning with LaTex \\[\n\\begin{align*}\nx&=y           &  w &=z              &  a&=b+c\\\\\n2x&=-y         &  3w&=\\frac{1}{2}z   &  a&=b\\\\\n-4 + 5x&=2+y   &  w+2&=-1+w          &  ab&=cb\n\\end{align*}\n\\]\nfrom source import Perceptron\nHello World !\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins Post\n\n\n\n\n\nTraining data to accurately classify penguins using LogisticRegression and DecisionTreeClassifier models!\n\n\n\n\n\nFeb 20, 2024\n\n\nDaniela Delgado\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow with one edit made by Daniela\n\n\n\n\n\n\nNo matching items"
  }
]