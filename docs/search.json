[
  {
    "objectID": "WarmUps/WarmUp9.html",
    "href": "WarmUps/WarmUp9.html",
    "title": "Daniela's CSCI 0451 Blog",
    "section": "",
    "text": "import torch\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n\nimport torch\n\nclass LinearModel:\n\n    def __init__(self):\n        self.w = None \n\n    def score(self, X):\n        \"\"\"\n        Compute the scores for each data point in the feature matrix X. \n        The formula for the ith entry of s is s[i] = &lt;self.w, x[i]&gt;. \n\n        If self.w currently has value None, then it is necessary to first initialize self.w to a random value. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            s torch.Tensor: vector of scores. s.size() = (n,)\n        \"\"\"\n        if self.w is None: \n            self.w = torch.rand((X.size()[1]))\n         # your computation here: compute the vector of scores s\n\n        scores = (X@self.w)\n        return scores\n\n    def predict(self, X):\n        \"\"\"\n        Compute the predictions for each data point in the feature matrix X. The prediction for the ith data point is either 0 or 1. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            y_hat, torch.Tensor: vector predictions in {0.0, 1.0}. y_hat.size() = (n,)\n        \"\"\"\n        scores = self.score(X)\n        y_hat = torch.where(scores &gt;= 0, torch.tensor(1.0), torch.tensor(0.0))\n        # or y_hat = (scores &lt; 0) * (1.0)\n        return y_hat \n\n\n\nclass Perceptron(LinearModel):\n\n    def loss(self, X, y):\n        \"\"\"\n        Compute the misclassification rate. A point i is classified correctly if it holds that s_i*y_i_ &gt; 0, \n        where y_i_ is the *modified label* that has values in {-1, 1} (rather than {0, 1}). \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n            y, torch.Tensor: the target vector.  y.size() = (n,). The possible labels for y are {0, 1}\n        \n        HINT: In order to use the math formulas in the lecture, \n        you are going to need to construct a modified set of targets and predictions \n        that have entries in {-1, 1} -- otherwise none of the formulas will work right! \n        An easy to to make this conversion is: \n        \n        y_ = 2*y - 1\n        \"\"\"\n\n        # replace with your implementation\n        #converting [0,1] to [-1, 1]\n        y_ = 2*y - 1\n        #find misclassified\n        missclassified = y_ * self.score(X) &lt;= 0\n        missclass_rate = torch.mean(1.0*missclassified)\n        return missclass_rate\n\n\n    def grad(self, X, y):\n        pass \n\n\n\np = Perceptron()\ns = p.score(X)\nl = p.loss(X, y)\nprint(l == 0.5)\n\ntensor(True)\n\n\n\nclass PerceptronOptimizer:\n\n    def __init__(self, model):\n        self.model = model \n    \n    def step(self, X, y):\n        \"\"\"\n        Compute one step of the perceptron update using the feature matrix X \n        and target vector y. \n        \"\"\"\n        pass"
  },
  {
    "objectID": "WarmUps/WarmUp3.html",
    "href": "WarmUps/WarmUp3.html",
    "title": "Daniela's CSCI 0451 Blog",
    "section": "",
    "text": "Part B:\nLet’s now imagine that the rapid COVID test does not just give a yes/no answer, but actually a score describing the patient’s likelihood of COVID on a scale from 0 to 1. What score is high enough to merit you staying home, according to your costs from Part A?\n\nimport numpy as np\n\nNUM_CASES  = 1000\nPREVALENCE = 0.1\nNOISE      = 2\n\ncases  = 1*(np.random.rand(NUM_CASES) &lt; PREVALENCE)\nscores = np.exp(cases + NOISE*(np.random.rand(NUM_CASES))) / np.exp(NOISE+1)\n\n\ndef t_cost (t):\n    cost_a = 0\n    cost_b = 0\n\n    for i in cases:\n        for j in scores:\n            if j &gt; t: #if false positive\n                if i == 0:\n                    cost_a += 1\n            elif j &lt; t: #if false negative\n                if i == 1:\n                    cost_b += 1\n    total_cost = 1 * cost_a + cost_b * 5\n    return total_cost\n\n#Using a for-loop or any other technique, conduct a search to find the value of t that minimizes the total cost\nmin_cost = float('inf')\nmin_t = None\n\nfor i in range(0, 1001):\n    tresh = i/1000 #divides the number by 100 to get a num between 0 and 1\n    cost = t_cost(tresh)\n    if cost &lt; min_cost:\n        min_cost = cost\n        min_t = tresh\nmin_t\n\n\n\n0.997"
  },
  {
    "objectID": "WarmUps/WarmUp2.html",
    "href": "WarmUps/WarmUp2.html",
    "title": "Daniela's CSCI 0451 Blog",
    "section": "",
    "text": "Graphing Decision Boundaries\nPart A:\nWe use the dot product equation \\(w_i x_i + w_j x_j = b\\) and substitute the values for w and b: \\[(1) * x_i + -\\dfrac 1 2 * x_j = \\dfrac 1 2\\] We make the equation into \\(y = mx + b\\) to plot it: \\[ x_j = 2 * x_i - 1\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#making range for x\nx_vals = np.linspace(-5, 10, num = 10)\n\n#solving for x2\nx2_vals = 2 * x_vals - 1\n\nplt.plot(x_vals, x2_vals, color = 'pink')\n\nplt.xlabel = ('x1')\nplt.ylabel = ('x2')\nplt.title = (\"Graph of 1(x1) + (-1/2)x2 = 1/2\")\nplt.grid (True, linestyle = '--')\nplt.show()\n\n\n\n\n\n\n\n\n\nPart B:\nWrite a quick Python function called linear_classify(x, w, b). w and x should both be 1d numpy arrays of the same length, and b should be a scalar. The function np.dot(x, w) will compute the inner product of x and w. Argument b should be a scalar number. Your function should return 0 if dot product of w and x &lt; b and 1 if the dot product of w and x &gt;= b.\n\nimport numpy as np\ndef linear_classify(x, w, b):\n    if  np.dot(w, x) &lt; b: \n        return 0\n    elif (w@x) &gt;= b: \n        return 1\n    \n#testing\n    #should return 1 since dot prod &gt; b\nw = np.array([1, 2, 3, 4])\nx = np.array([2, 6, -1, 0])\nb = 3\n    #should return 0 since dot prod &lt; b\nw1 = np.array([1, 2, 3, 4])\nx1 = np.array([2, 6, -1, 0])\nb1 = 30\n\nw2 = np.array([2, -10, -5])\nx2 = np.array([2, -1, 0])\nb2 = 10\n\ntesting = linear_classify(w, x, b)\ntesting2 = linear_classify(w1, x1, b1)\ntesting3 = linear_classify(w2, x2, b2)\n\nprint('Test 1: ', testing)\nprint('Test 2: ', testing2)\nprint('Test 2: ', testing3)\n\n\nTest 1:  1\nTest 2:  0\nTest 2:  1\n\n\nPart C:\nMake a sketch of the curve in the nonnegative quadrant defined by the equation given.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#making range for x\nx_vals = np.linspace(0, 10)\nx1_val = np.sqrt(1 - 1/4 * (x_vals**2))\n\nplt.plot(x_vals, x1_val, color = 'pink')\nplt.xlabel = ('x')\nplt.ylabel = ('x1')\nplt.title = ('Curve in Nonnegative Quadrant')\nplt.grid (True, linestyle = '--')\nplt.show()\n\n/var/folders/33/b3h8vrcd0ss7_3q4gmmnscl80000gn/T/ipykernel_2446/4100891223.py:6: RuntimeWarning: invalid value encountered in sqrt\n  x1_val = np.sqrt(1 - 1/4 * (x_vals**2))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins Post\n\n\n\n\n\nTraining data to accurately classify penguins using LogisticRegression and DecisionTreeClassifier models!\n\n\n\n\n\nFeb 20, 2024\n\n\nDaniela Delgado\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Decision Making\n\n\n\n\n\nTraining data to predict if a person will default on a loan by finding a threshold using a LogisticRegression model!\n\n\n\n\n\nFeb 20, 2024\n\n\nDaniela Delgado\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow with one edit made by Daniela\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/women-in-DS/index.html",
    "href": "posts/women-in-DS/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/women-in-DS/index.html#math",
    "href": "posts/women-in-DS/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "Figure 1: An image of the Earth!\nFigure 1 is an image of the Earth.\nFigure 2 is a comic by Randall Munroe.\n\\[ F = ma \\]\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} }\ne^{-\\frac{1}{2}\n\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\n}\n\\]\nAligning with LaTex \\[\n\\begin{align*}\nx&=y           &  w &=z              &  a&=b+c\\\\\n2x&=-y         &  3w&=\\frac{1}{2}z   &  a&=b\\\\\n-4 + 5x&=2+y   &  w+2&=-1+w          &  ab&=cb\n\\end{align*}\n\\]\nfrom source import Perceptron\nHello World !\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/classifying-penguins/index.html",
    "href": "posts/classifying-penguins/index.html",
    "title": "Classifying Palmer Penguins Post",
    "section": "",
    "text": "Abstract\nIn this blog post, we will predict the species of a penguin based on its measurements and aim to achieve 100% testing accuracy with my select choice in model. First, I will do data preparation where I will clean the dataset by removing unnecessary columns, labels, and, of course, the species column. Then, by looking at this new dataset, I will make graphs using seaborn based on the information given to see if I can spot any patterns that can help identify a penguin’s species. In my graphs, I look at flipper length in relation to body mass, culmen depth and length in relation to the penguin’s sex, and the correlation between the penguin’s culmen depth and lengh with the flipper length. Then I made a summary table where I found the mean and the standard deviation of the culmen length and depth, the flipper length, and the body mass of the penguins on each island based on their sex to see the areas where penguins had a closer correlation to. Then, using LogisticRegression and DecisionTreeClassifier, we will find the best mean score and its corresponding column out of every set of 3 features- two quantitative and one qualitative features. We will also test this on our testing data. Afterwards, we will plot a graph panel of decision regions for the classifiers where we will see the DTC predictions for the penguin species based on the best column we got from finding the best mean with the DTC. Finally, we will see the errors our prediction model made in comparison to the actual data by looking at a color-coded confusion matrix.\nAs the results will show, the DecisionTreeClassifier model worked better than the LogisticRegression model yet, in the test data, LogisticRegression returned a better output with 100% while DTC returned about 98.5% accuracy. This can be due to the depth limit placed on the DTC in comparison to no limit set on the LR model. Additionally, the confusion matrix showed the DTC made a very small margin error with one penguin being mislabeled. The prediction DTC classified this penguin as Chinstrap when it was actually a Gentoo penguin. Overall, the prediction model was accurate.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"]) #fit encoder on 'Species'\n\ndef prepare_data(df):\n  #removing columns that are not needed\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  #removing when sex = .\n  df = df[df[\"Sex\"] != \".\"]\n  #removing N/A labels\n  df = df.dropna()\n  #print(df)\n  #turns the labels in 'Species' to a number\n  y = le.transform(df[\"Species\"])\n  #print(y)\n  #removing 'Species' col bc now held by y\n  df = df.drop([\"Species\"], axis = 1)\n  #converted into “one-hot encoded” 0-1 columns\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nPart 1: Exploring the data:\nHere, I constructed three interesting displayed figures and one summary table that will help draw conclusions about what features I can use for the model (DTC model).\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(data = X_train, x = 'Flipper Length (mm)', y = 'Body Mass (g)', hue = 'Flipper Length (mm)')\n\nplt.title('Scatterplot of Flipper Length in Relation to Body Mass')\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we see the correlation between the flipper length and the body mass of penguins as the data is grouped by the flipper length. In the graph, the correlation between both seems to be positive as the longer the flipper length, the higher the body mass. This can help define a species as maybe some penguin species are smaller, which would allow us to know their body mass is lighter, thus their flipper length is smaller as well. This can be used in my model as it can use the weigths/sizes of penguins to possibibly help distinguish the species, along with the help of looking at other features to understand the outlier points in this set.\n\nsns.scatterplot(data = X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Sex_FEMALE')\nplt.title('Culmen Length and Depth Based on Gender')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we can see the culmen depth and length in relation to the penguin’s sex. The female culmens tend to be on the lower half of the graph. There also seems to be three different clusters: one on the middle left averaging on the point (40,18), another one starting at (45,16) and expanding upwards right to (50,20), and the final one being on the mid-lower part of the graph at about (45,14) to (50,16). In these three clusters, the female’s culmen dimensions are smaller than the men’s. This can tell us that the species of penguins all have different culmen lengths and depths, and they vary by sex. Thus, the penguin species can be distinguishable by their culmen sizes. This information can be useful in modeling as we can see if the culmen length and culmen depth compared to the gender play a role in distinguishing the species.\n\nsns.scatterplot(data = X_train, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Flipper Length (mm)')\nplt.title('Correlation Between Culmen Length and Depth, and Flipper Length')\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, we can see the correlation between the flipper length and the culmen depth/length. We can see how the deeper the culmen depth goes, the smaller the flipper and culmen lengths are. If the penguin has a medium to high range in culmen length with a lower depth, then the flipper lenght is longer. However, if both the culmen length and depth are high, then the flipper length is in about the middle size. This information can be used in modeling as we can see if the species of the penguins are identifiable based on the different correlations between the culmen dimensions and the flipper length.\n\nmeanTable = X_train.groupby(['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Sex_MALE', 'Sex_FEMALE']).aggregate(\n    {'Culmen Length (mm)' : ['mean', 'std'], 'Culmen Depth (mm)' : ['mean', 'std'], 'Flipper Length (mm)' : ['mean', 'std'], 'Body Mass (g)' : ['mean', 'std']})\nmeanTable\n\n\n\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nSex_MALE\nSex_FEMALE\n\n\n\n\n\n\n\n\n\n\n\n\nFalse\nFalse\nTrue\nFalse\nTrue\n37.537500\n2.111200\n17.475000\n0.863713\n188.000000\n4.661902\n3309.375000\n251.143219\n\n\nTrue\nFalse\n40.961111\n3.273268\n19.300000\n1.058856\n196.000000\n5.455704\n4100.000000\n370.611577\n\n\nTrue\nFalse\nFalse\nTrue\n43.041176\n5.368843\n17.629412\n0.767670\n190.784314\n5.923896\n3471.078431\n267.225212\n\n\nTrue\nFalse\n46.176087\n5.930081\n19.058696\n0.971271\n196.782609\n7.828475\n4027.173913\n328.541647\n\n\nTrue\nFalse\nFalse\nFalse\nTrue\n43.423077\n4.061703\n15.093846\n1.727795\n206.507692\n11.936189\n4356.538462\n655.569244\n\n\nTrue\nFalse\n46.595000\n4.511583\n16.696667\n1.738007\n212.366667\n15.248192\n5076.250000\n721.116498\n\n\n\n\n\n\n\nIn this table, we can see the mean and standard deviation values for the culmen length and depth, the flipper length, and the body mass of the penguins on each island based on their sex. This table can be helpful as we can see the how the dimensions differ based on the island the penguins are on and their sex. These can be a telling factor on the kind of species they are by seeing the grouping they fall into and if the standard deviation is too big, then we know there are more outliers that are not close to the mean. Thus, this shows that there might not be a correlatioin with that said data.\nPart 2: Choosing Features\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score #use this isntead of for-loop\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\n\n#warnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category = FutureWarning)\n\n\nscore_counter = 0\n# these are not actually all the columns: you'll\n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Egg Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    print(cols)\n    # training LR model and scoring it\n    LR = LogisticRegression()\n    #using cross validation on LR to avoid overfitting\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5) #training on remaining 80% of data\n\n    if cv_scores_LR.mean() &gt; score_counter:\n      #updating the best score and columns of that score\n      score_counter = cv_scores_LR.mean()\n      col_best = cols\n\nprint('Best Score: ', score_counter)\nprint('Best Three Columns: ', col_best)\n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Flipper Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Flipper Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Flipper Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Length (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\n['Flipper Length (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\nBest Score:  0.9961538461538462\nBest Three Columns:  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\n# LR with the selected better columns from the code above\ncols = col_best\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.99609375\n\n\nNow to test LogisticRegression:\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nUsing DecisionTreeClassifier instead of Logistic Regression, here is another model choice:\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score #use this isntead of for-loop\nfrom sklearn.tree import DecisionTreeClassifier\n\nscore_counter = 0\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Egg Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols:\n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols =  list(pair) + qual_cols\n    print(cols)\n\n    for i in range(1,20):\n      #setting the max depth to be between 1-19 and iterating through each one\n      DTC = DecisionTreeClassifier(max_depth=i)\n      #using cross validation on DTC to avoid overfitting\n      cv_scores_DTC = cross_val_score(DTC, X_train[cols], y_train, cv = 5)\n      #updating the best score and columns of that score\n      if cv_scores_DTC.mean() &gt; score_counter:\n        score_counter = cv_scores_DTC.mean()\n        col_best = cols\n    \nprint('Best Score: ', score_counter)\nprint('Best Three Columns: ', col_best)\nprint('At depth: ', i)\n    \n\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Flipper Length (mm)', 'Body Mass (g)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Flipper Length (mm)', 'Body Mass (g)', 'Sex_FEMALE', 'Sex_MALE']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Flipper Length (mm)', 'Body Mass (g)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Length (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Depth (mm)', 'Flipper Length (mm)', 'Stage_Adult, 1 Egg Stage']\n['Culmen Depth (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\n['Flipper Length (mm)', 'Body Mass (g)', 'Stage_Adult, 1 Egg Stage']\nBest Score:  0.9765460030165913\nBest Three Columns:  ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\nAt depth:  19\n\n\n\n#Feeding the best cols\ncols = col_best\n\nDTC = DecisionTreeClassifier(max_depth=i)\nDTC.fit(X_train[cols], y_train)\nDTC.score(X_train[cols], y_train)\n\n1.0\n\n\nNow to test the data we do:\n\n#download the test data set \ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n#prepping data\nX_test, y_test = prepare_data(test)\nDTC.score(X_test[cols], y_test)\n\n0.9852941176470589\n\n\nNow we are going to plot a panel of decision regions for the classifiers:\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    #sets first two columns, which are quantitative, of X_train[cols]\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    #says remaining columns are one-hot qualitative columns\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(DTC, X_train[cols], y_train)\n\n\n\n\n\n\n\n\nFinally, we will show a confusion matrix on the DTC model, which will be evaluated on the test set.\n\nfrom sklearn.metrics import confusion_matrix\n\nactual = y_test\npredict = DTC.predict(X_test[cols])\n\nconf_matrix = confusion_matrix(actual, predict)\n\nprint(conf_matrix)\n\n[[31  0  0]\n [ 0 10  1]\n [ 0  0 26]]\n\n\nIn this confusion matrix, we see the error is low, but it is seen with the Gentoo and Chinstrap penguins. The data predicted one penguin to be Chinstrap when it was actually a Gentoo penguin. With this information, I do not believe there was an error greater than others as that seems to be the only error.\nDiscussion\nIn this blog post, I found that using a DecisionTreeClassifier model worked better than the LogisticRegression model as seen by the best score prints, yet, in the test data, LogisticRegression returned a better output with 100% while DTC returned about 98.5% accuracy. I believe this is because LR does not have a limit on the iterations while DTC has a set depth, which can limit it from getting to 100% accuracy. Furthermore, the graphs explored in the beginning did help classify the data to guess the correct species, specifically the penguin’s culmen dimensions correlating to its sex as that ended up being the best column outcome for the DTC! The three clusters I saw in that graph also proved to help identify the species as seen in the plotted graph panel of decision regions for the species classifiers. Lastly, as we can see from the confusion matrix, we can conclude the DTC prediction model is accurate.\nThrough the coding process, I learned and felt more comfortable in working with seaborn and the different graphs available to make since, in part one, I originally created about seven different graphs from pairplots, to boxplots, to bar graphs before deciding on my final three. Additionally, I learned what the DecisionTreeClassifies does and the different methods it has to help with prediction and other needs. One more thing I learned was what a confusion matrix was. I had to do research on it as I had no idea what the purpose of it was nor how to use the data I had to create it. However, with the help of Geeks4Geeks and the scikitlearn website on confusion matrices, I understood the purpose and what data to look at."
  },
  {
    "objectID": "posts/optimal-decision-making/index.html",
    "href": "posts/optimal-decision-making/index.html",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "Introduction\n\nIn this blog post, I start off by making different graphs and tables to see if there is any correlation and patterns between a person’s information and seeing how the interest rate is affected by this information. Then, I used logistic regression to construct a score function and threshold for predicting whether a prospective borrower is likely to default on a given loan. Looping through the max and min scores, I found the best threshold to maximize the bank’s profit by calculating how much the bank will gain or lose based on if the person with that score defaults or not from paying the loan. In my test results, I got that the best threshold was about -0.175 with the bank’s expected profit per borrower to be about $1168.39.\nIn seeing if certain groups have more difficulty than others in getting a loan because of defaulting based on the threshold proposed from the system, all of the borrowers have a decision, or probability, of defaulting based on the threshold. There might be an error or missing step in my calculations.\n\n\n\nPart A: Grab the Data\n\n# downloading training data\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nPart B: Explore The Data\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(data = df_train, x = 'loan_int_rate', y = 'loan_percent_income', hue = 'cb_person_default_on_file')\nplt.title(\"Loan interest rate and loan percent income based on person defaulting\")\nplt.show()\n\n\n\n\n\n\n\n\nIn this scatterplot, we can see the how the loan interest rate is affected by seeing if the person defaulted previously on a loan and the person’s percent income. The loan interest rate increases if the person has a record of defaulting on a loan.\n\nsns.boxplot(data = df_train, x = 'person_home_ownership', y = 'loan_int_rate')\nplt.title(\"Interest Rate Based on a Person's Home Ownership\")\nplt.show()\n\n\n\n\n\n\n\n\nThis graph further explores how the loan interest rate differs and compares based on a person’s home ownership. We can see that a person who has a mortgage has a lower mean for the interest rate, while a person renting and those with other forms of home ownership have higher loan interest rates on average.\n\nmeanTable = df_train.groupby(['loan_intent', 'cb_person_default_on_file']).aggregate({'person_age': ['mean', 'std'], 'person_income' : ['mean', 'std'], 'cb_person_cred_hist_length' : ['mean', 'std'], 'loan_int_rate': ['mean', 'std']})\nmeanTable\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\ncb_person_cred_hist_length\nloan_int_rate\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nloan_intent\ncb_person_default_on_file\n\n\n\n\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\nN\n27.523949\n5.822429\n66504.540596\n54849.658619\n5.644276\n3.915509\n10.216206\n3.008325\n\n\nY\n27.883289\n6.379513\n67551.327586\n58425.810386\n5.928382\n4.121033\n14.565023\n1.758878\n\n\nEDUCATION\nN\n26.505755\n6.058923\n64403.688983\n40406.252292\n5.074231\n3.572837\n10.273948\n2.967930\n\n\nY\n27.047126\n6.009493\n61127.258621\n35589.171553\n5.471264\n3.910798\n14.357234\n1.650609\n\n\nHOMEIMPROVEMENT\nN\n29.088624\n5.637893\n72891.920750\n49281.092741\n6.512995\n3.964697\n10.358453\n3.158075\n\n\nY\n28.529730\n5.966250\n73886.228829\n52241.077202\n6.079279\n3.974765\n14.576099\n1.739466\n\n\nMEDICAL\nN\n27.927044\n6.376993\n61528.271447\n48179.985735\n5.912201\n4.004978\n10.281730\n2.989114\n\n\nY\n28.061628\n6.322874\n60326.900000\n59500.584963\n5.919767\n4.109030\n14.531599\n1.718297\n\n\nPERSONAL\nN\n28.361028\n7.562245\n68244.272206\n113892.066168\n6.190763\n4.709577\n10.280876\n2.981917\n\n\nY\n27.933244\n6.696901\n67221.606142\n57537.660727\n5.958611\n4.295520\n14.536935\n1.737584\n\n\nVENTURE\nN\n27.589281\n6.216285\n65544.232157\n46928.520701\n5.745882\n3.957178\n10.189561\n2.935957\n\n\nY\n27.585551\n5.808216\n68787.400507\n82746.627032\n5.735108\n4.021774\n14.544407\n1.740218\n\n\n\n\n\n\n\nIn this table, part of the data is summarized by finding the mean and the standard deviation of a person’s age, their income, their credit history length, and their given loan interest rate based on their loan intent and whether or not they defaulted previously on a loan. This table is helpful as we can see what factors play into having a higher interest rate, and we can see how age, income, or their credit history length varies based on defaulting on a loan and their loan intent. The standard deviation is important to see how much of the data falls into or out of the calculated mean.\n\n\nPart C: Building a Model:\nConstruct a score function and threshold for predicting whether a prospective borrower is likely to default on a given loan. You may use all the features in the data except loan_grade and the target variable loan_status.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"]) #fit encoder on 'loan_status'\n\ndef prepare_data(df):\n  #removing columns that are not needed\n  df = df.drop([\"loan_grade\"], axis = 1)\n  #removing N/A labels\n  df = df.dropna()\n  #print(df)\n  #turns the labels in 'loan_status' to a number\n  y = le.transform(df[\"loan_status\"])\n  #print(y)\n  #removing 'loan_status' col bc now held by y\n  df = df.drop([\"loan_status\"], axis = 1)\n  #converted into “one-hot encoded” 0-1 columns\n  df = pd.get_dummies(df)\n  return df, y\n\n\nX_train, y_train = prepare_data(df_train)\n\n\n\ny_train\n\narray([0, 0, 1, ..., 0, 0, 1])\n\n\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score #use this isntead of for-loop\nfrom sklearn.linear_model import LogisticRegression\nimport random\n\nquant_cols = ['person_income', 'loan_amnt', 'loan_int_rate']\n\nscore_counter = 0\n\nLR = LogisticRegression()\n\n#using cross validation on LR to avoid overfitting\ncv_scores_LR = cross_val_score(LR, X_train[quant_cols], y_train, cv = 5) #training on remaining 80% of data\nif cv_scores_LR.mean() &gt; score_counter:\n      #updating the best score and columns of that score\n      score_counter = cv_scores_LR.mean()\n\nprint('Best Score: ', score_counter)\n\nBest Score:  0.8117172718507574\n\n\n\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\n...\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\npaid_gain\ndefault_loss\nScores\ndecision\n\n\n\n\n1\n27\n98000\n3.0\n11750\n13.47\n0.12\n6\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n4613.567568\n-6997.533847\n-2.724145\nFalse\n\n\n2\n22\n36996\n5.0\n10000\n7.51\n0.27\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n2044.334031\n-6426.108799\n-0.435472\nFalse\n\n\n3\n24\n26000\n2.0\n1325\n12.87\n0.05\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n493.650464\n-795.445199\n-0.913722\nFalse\n\n\n4\n29\n53004\n2.0\n15000\n9.63\n0.28\n10\nTrue\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n4028.690420\n-9390.333437\n-0.552180\nFalse\n\n\n6\n21\n21700\n2.0\n5500\n14.91\n0.25\n2\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n2430.522429\n-3211.752128\n-0.294372\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n8.0\n3000\n7.29\n0.02\n17\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n593.840622\n-1932.967484\n-5.766362\nFalse\n\n\n26060\n23\n48000\n1.0\n4325\n5.42\n0.09\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n623.093432\n-2849.295748\n-1.486665\nFalse\n\n\n26061\n22\n60000\n0.0\n15000\n11.71\n0.25\n4\nFalse\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n5017.300210\n-9143.682505\n-0.836032\nFalse\n\n\n26062\n30\n144000\n12.0\n35000\n12.68\n0.24\n8\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n12819.204794\n-21064.871625\n-2.113038\nFalse\n\n\n26063\n25\n60000\n5.0\n21450\n7.29\n0.36\n4\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n4245.960448\n-13820.717511\n-0.148728\nFalse\n\n\n\n\n22907 rows × 23 columns\n\n\n\n\ny_train\n\narray([0, 0, 1, ..., 0, 0, 1])\n\n\n\nLR.fit(X_train[quant_cols], y_train)\nw = LR.coef_\nLR.score(X_train[quant_cols], y_train)\n\n\n0.8080062862880342\n\n\n\nw\n\narray([[-4.05735976e-05,  1.06558819e-04,  9.49045880e-08]])\n\n\n\n#finding the scores\nx = X_train[quant_cols]\nx\n\n\n\n\n\n\n\n\nperson_income\nloan_amnt\nloan_int_rate\n\n\n\n\n1\n98000\n11750\n13.47\n\n\n2\n36996\n10000\n7.51\n\n\n3\n26000\n1325\n12.87\n\n\n4\n53004\n15000\n9.63\n\n\n6\n21700\n5500\n14.91\n\n\n...\n...\n...\n...\n\n\n26059\n150000\n3000\n7.29\n\n\n26060\n48000\n4325\n5.42\n\n\n26061\n60000\n15000\n11.71\n\n\n26062\n144000\n35000\n12.68\n\n\n26063\n60000\n21450\n7.29\n\n\n\n\n22907 rows × 3 columns\n\n\n\n\nscores = x@w.T\nscores\n\n\n\n\n\n\n\n\n0\n\n\n\n\n1\n-2.724145\n\n\n2\n-0.435472\n\n\n3\n-0.913722\n\n\n4\n-0.552180\n\n\n6\n-0.294372\n\n\n...\n...\n\n\n26059\n-5.766362\n\n\n26060\n-1.486665\n\n\n26061\n-0.836032\n\n\n26062\n-2.113038\n\n\n26063\n-0.148728\n\n\n\n\n22907 rows × 1 columns\n\n\n\n\n\nPart D: Finding a Threshold\nFind a threshold that maximizes the bank’s profit.\n\nimport numpy as np\n\n#calculate paid gain and loss here and add these array values in the columns of the table\n#vectorization\nint_rate_decimal = np.divide(X_train['loan_int_rate'], 100)\nX_train['paid_gain'] = X_train['loan_amnt'] * (1 + 0.25 * int_rate_decimal)**10 - X_train['loan_amnt']\nX_train['default_loss'] = X_train['loan_amnt'] * (1 + 0.25 * int_rate_decimal)** - 1.7 * X_train['loan_amnt']\nX_train['Scores'] = scores\n\n\nX_train['Scores']\n\n1       -2.724145\n2       -0.435472\n3       -0.913722\n4       -0.552180\n6       -0.294372\n           ...   \n26059   -5.766362\n26060   -1.486665\n26061   -0.836032\n26062   -2.113038\n26063   -0.148728\nName: Scores, Length: 22907, dtype: float64\n\n\n\nall_total_profit = []\nbest_profit = 0\ntotal_t_profit = 0\n\nfor threshold in np.linspace(scores.min(), scores.max(), 100): #np.linspace(scores.min(), scores.max(), 100)\n    #new col in for loop that makes the decision \"decision col\" checks if score is larger than threshold\n    X_train['decision'] = X_train['Scores'] &gt;= threshold[0] #returns bool threshold[0] with np.linspace\n\n\n    t_profit = (1 - X_train['decision']) * (X_train['paid_gain'] * (1 - y_train) + X_train['default_loss'] * (y_train))\n\n    # t_profit = (1-threshold) * (X_train['paid_gain'] + threshold * X_train['default_loss'])\n    \n    # print(f\"{X_train['paid_gain']= }\")\n    # print(f\"{X_train['default_loss']= }\")\n\n    #calculating mean profit\n    all_total_profit.append(t_profit.mean())\n\n    #return highest threshold based on highest gain\n    if t_profit.mean() &gt; best_profit:\n        best_profit = t_profit.mean()\n        best_t = threshold[0] #threshold[0] with np.linspace\n        profit_per_borrower = t_profit.mean()\n\n\n        #find probability and loop through thresholds\nprint(total_t_profit)\nprint('highest profit: ', best_profit)\nprint('best threshold that optimizes profit for the bank: ', best_t)\nprint('bank’s expected profit per borrower: ', profit_per_borrower)\n\n0\nhighest profit:  1158.6189787929175\nbest threshold that optimizes profit for the bank:  -0.9466451695144542\nbank’s expected profit per borrower:  1158.6189787929175\n\n\n\n#threshold vs profit (all_total_profit)\nplt.plot(np.linspace(scores.min(), scores.max(), 100), all_total_profit)\nplt.gca().set(xlim=(-7,None))\nplt.grid(True)\nplt.axvline(x = best_t, color = 'red', linestyle = '--', lw = 1)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Total Profit\")\nplt.title(\"Threshold vs. Total Profit\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPart E: Evaluate Your Model from the Bank’s Perspective\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\nLR.score(X_test[quant_cols], y_test)\nx = X_test[quant_cols]\ntest_scores = x@w.T\n\nFinding threshold of testing set:\n\nimport numpy as np\n\n#calculate paid gain and loss here and add these array values in the columns of the table\n#vectorization\nint_rate_decimal = np.divide(X_test['loan_int_rate'], 100)\nX_test['paid_gain'] = X_test['loan_amnt'] * (1 + 0.25 * int_rate_decimal)**10 - X_test['loan_amnt']\nX_test['default_loss'] = X_test['loan_amnt'] * (1 + 0.25 * int_rate_decimal)**3 - 1.7 * X_test['loan_amnt']\nX_test['Scores'] = test_scores\n\n\nall_total_profit = []\nbest_profit = 0\ntotal_t_profit = 0\n\nfor threshold in np.linspace(test_scores.min(), test_scores.max(), 100):\n    #new col in for loop that makes the decision \"decision col\" checks if score is larger than threshold\n    X_test['decision'] = X_test['Scores'] &gt; threshold[0] #returns bool threshold[0] with np.linspace\n\n    t_profit = (1 - X_test['decision']) * (X_test['paid_gain'] * (1 - y_test) + X_test['default_loss'] * (y_test))    \n\n    #calculating mean profit\n    all_total_profit.append(t_profit.mean())\n\n    #return highest threshold based on highest gain\n    if t_profit.mean() &gt; best_profit:\n        best_profit = t_profit.mean()\n        best_t = threshold[0] #threshold[0] with np.linspace\n        profit_per_borrower = t_profit.mean()\n\n\n        #find probability and loop through thresholds\nprint(total_t_profit)\nprint('highest profit: ', best_profit)\nprint('best threshold that optimizes profit for the bank: ', best_t)\nprint('bank’s expected profit per borrower: ', profit_per_borrower)\n\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n6511    True\n6513    True\n6514    True\n6515    True\n6516    True\nName: decision, Length: 5731, dtype: bool\n0        True\n1        True\n2       False\n3        True\n4       False\n        ...  \n6511     True\n6513     True\n6514     True\n6515    False\n6516     True\nName: decision, Length: 5731, dtype: bool\n0       False\n1        True\n2       False\n3       False\n4       False\n        ...  \n6511    False\n6513     True\n6514     True\n6515    False\n6516    False\nName: decision, Length: 5731, dtype: bool\n0       False\n1       False\n2       False\n3       False\n4       False\n        ...  \n6511    False\n6513    False\n6514    False\n6515    False\n6516    False\nName: decision, Length: 5731, dtype: bool\n0       False\n1       False\n2       False\n3       False\n4       False\n        ...  \n6511    False\n6513    False\n6514    False\n6515    False\n6516    False\nName: decision, Length: 5731, dtype: bool\n0       False\n1       False\n2       False\n3       False\n4       False\n        ...  \n6511    False\n6513    False\n6514    False\n6515    False\n6516    False\nName: decision, Length: 5731, dtype: bool\n0\nhighest profit:  1168.3899694144973\nbest threshold that optimizes profit for the bank:  -0.17519863260740465\nbank’s expected profit per borrower:  1168.3899694144973\n\n\n\n#threshold vs profit (all_total_profit)\nplt.plot(np.linspace(test_scores.min(), test_scores.max(), 100), all_total_profit)\nplt.gca().set(xlim=(-7,None))\nplt.grid(True)\nplt.axvline(x = best_t, color = 'red', linestyle = '--', lw = 1)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Total Profit\")\nplt.title(\"Threshold vs. Total Profit\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPart F: Evaluate Your Model From the Borrower’s Perspective\nIs it more difficult for people in certain age groups to access credit under your proposed system?\n\nimport seaborn as sns\n\nX_test['age_cat'] = pd.cut(X_test['person_age'], range(0,70, 10))\n\napproval_rate_percent = X_test.groupby('age_cat')['decision'].mean() * 100\nplt.figure(figsize=(11,5))\nsns.barplot(X_test, x = approval_rate_percent.index, y = approval_rate_percent.values)\nplt.xlabel('Age Group')\nplt.ylabel('Approval Rate Percent')\nplt.grid(True)\nplt.show()\n#(X_test['person_age'], y_test)\n\n/var/folders/1j/mlsnh9t96n70j2n5mmm9h9780000gn/T/ipykernel_11168/2421308665.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  approval_rate_percent = X_test.groupby('age_cat')['decision'].mean() * 100\n/Users/ddelgado/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n\n\n\n\n\n\n\nSince, all of my decision column has the boolean true, they are all 0.0 floats. Therefore, no one age group has more difficulty in getting a loan in the proposed system.\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\nloan_intent_rate = X_test.groupby(['loan_intent_EDUCATION', 'loan_intent_MEDICAL', 'loan_intent_VENTURE']).aggregate({'decision' : 'mean', 'cb_person_default_on_file_N' : 'mean', 'cb_person_default_on_file_Y' : 'mean'})\nprint(loan_intent_rate)\n\n                                                               decision  \\\nloan_intent_EDUCATION loan_intent_MEDICAL loan_intent_VENTURE             \nFalse                 False               False                     0.0   \n                                          True                      0.0   \n                      True                False                     0.0   \nTrue                  False               False                     0.0   \n\n                                                               cb_person_default_on_file_N  \\\nloan_intent_EDUCATION loan_intent_MEDICAL loan_intent_VENTURE                                \nFalse                 False               False                                   0.809770   \n                                          True                                    0.816390   \n                      True                False                                   0.835974   \nTrue                  False               False                                   0.816327   \n\n                                                               cb_person_default_on_file_Y  \nloan_intent_EDUCATION loan_intent_MEDICAL loan_intent_VENTURE                               \nFalse                 False               False                                   0.190230  \n                                          True                                    0.183610  \n                      True                False                                   0.164026  \nTrue                  False               False                                   0.183673  \n\n\nSince the decision column is all zeros, the comparison here is that the approval rate, or the decision, is less than the actual default rate for each education, medical and venture groups.\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n#income_groups = pd.cut(X_test['person_income'], range(4800,1782000, 10))\napproval_rate_income = X_test.groupby(pd.cut(X_test['person_income'], 25))['decision'].mean()\nprint(approval_rate_income)\n\nperson_income\n(3022.8, 75888.0]         0.0\n(75888.0, 146976.0]       0.0\n(146976.0, 218064.0]      0.0\n(218064.0, 289152.0]      0.0\n(289152.0, 360240.0]      0.0\n(360240.0, 431328.0]      0.0\n(431328.0, 502416.0]      0.0\n(502416.0, 573504.0]      0.0\n(573504.0, 644592.0]      NaN\n(644592.0, 715680.0]      0.0\n(715680.0, 786768.0]      0.0\n(786768.0, 857856.0]      0.0\n(857856.0, 928944.0]      NaN\n(928944.0, 1000032.0]     NaN\n(1000032.0, 1071120.0]    NaN\n(1071120.0, 1142208.0]    NaN\n(1142208.0, 1213296.0]    0.0\n(1213296.0, 1284384.0]    NaN\n(1284384.0, 1355472.0]    NaN\n(1355472.0, 1426560.0]    NaN\n(1426560.0, 1497648.0]    NaN\n(1497648.0, 1568736.0]    NaN\n(1568736.0, 1639824.0]    NaN\n(1639824.0, 1710912.0]    NaN\n(1710912.0, 1782000.0]    0.0\nName: decision, dtype: float64\n\n\n/var/folders/1j/mlsnh9t96n70j2n5mmm9h9780000gn/T/ipykernel_11168/4155170202.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  approval_rate_income = X_test.groupby(pd.cut(X_test['person_income'], 25))['decision'].mean()\n\n\nA person’s income level does not impact the ease in which a person can access credit under the decision system as the decision says the mean is 0.0\n\n\nConclusion\nIn this blog post, what I learned was that something in my decisions might be calculated wrong or I might be missing something as all of my decisions say they are True for defaulting. If everything is correct, I learned that everyone is set to default on the loan, so there really is no difference between groups of people.\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\n\nAssuming my data showed that people seeking loans for medical expenses have higher rates of defaulting, I believe that it is fair for them to have more difficulty in obtaining access to credit. In this sense, it is fair because the bank has to think in terms of itself and if the people who seek medical expenses cannot pay the loan back, it is fair for the bank to deny their loan as it is money the bank will be losing. If something is fair, it does not mean it is morally correct. In this case, fairness is based on the system proposed and if a borrower falls into the given threshold for loan acceptance based on different factors. However, I would say it is not fair according to morals to deny the loan for their medical expenses as they need the money to get treatment/medication in order to remain alive."
  },
  {
    "objectID": "posts/goals-setting/goal-setting.html",
    "href": "posts/goals-setting/goal-setting.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Daniela Delgado\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would really like to grow in the implementation and social responsibility areas as these are where my interests lie. I want to understand machine learning algorithms and be able to compare algorithms to know which ones work better for certain tasks. Additionally, even with this great technology, it falls in our hands to be able to go against biases and for these models to be ethical. As a POC woman in STEM, I am aware of the discrimination, stereotypes, and limitations women and people of color face in the field. Thus, I would like to understand where the faults lie in machine learning and be able to learn and think about ways to eliminate/improve the biases to avoid them growing through and because of technology.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nFor the blog posts, I want to achieve at least five “No Revisions Suggested” posts and at least one post for each of the sections, except at least two for “Implementation” and “Social Responsibility” as these are the topics that most interest me.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI would like to use the “pass” option at most two times during the semester when it is my turn to present. Additionally, I would like to ask questions to whoever is presenting the warmup when I need more clarification, which might be often. I often struggle to ask questions out loud, so I hope this creates an added incentive for me to ask others for clarifying help. I will also attend student/peer help hours at least twice a week to ask clarifying/curious questions over the material. If there is a study group, I would like to attend a session (once a week at least if it is helpful the first time). I will complete all core readings prior to each class periods and take notes on them.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI will submit all of the project milestones on time. Additionally, when working in the group, we will set a regular time each week, multiple times a week as needed, to work with the group on the project. I will also communicate with my group in a clear and timely manner. I will write the sections assigned to me for the project report and present the portion assigned to me for the final presentation.\nThe topics I would like to explore in a project would be making predictions, classifying music or items, or a recognition projection (such as music or emotions or an image recognition)."
  },
  {
    "objectID": "posts/goals-setting/goal-setting.html#what-youll-learn",
    "href": "posts/goals-setting/goal-setting.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI would really like to grow in the implementation and social responsibility areas as these are where my interests lie. I want to understand machine learning algorithms and be able to compare algorithms to know which ones work better for certain tasks. Additionally, even with this great technology, it falls in our hands to be able to go against biases and for these models to be ethical. As a POC woman in STEM, I am aware of the discrimination, stereotypes, and limitations women and people of color face in the field. Thus, I would like to understand where the faults lie in machine learning and be able to learn and think about ways to eliminate/improve the biases to avoid them growing through and because of technology."
  },
  {
    "objectID": "posts/goals-setting/goal-setting.html#what-youll-achieve",
    "href": "posts/goals-setting/goal-setting.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nFor the blog posts, I want to achieve at least five “No Revisions Suggested” posts and at least one post for each of the sections, except at least two for “Implementation” and “Social Responsibility” as these are the topics that most interest me.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI would like to use the “pass” option at most two times during the semester when it is my turn to present. Additionally, I would like to ask questions to whoever is presenting the warmup when I need more clarification, which might be often. I often struggle to ask questions out loud, so I hope this creates an added incentive for me to ask others for clarifying help. I will also attend student/peer help hours at least twice a week to ask clarifying/curious questions over the material. If there is a study group, I would like to attend a session (once a week at least if it is helpful the first time). I will complete all core readings prior to each class periods and take notes on them.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI will submit all of the project milestones on time. Additionally, when working in the group, we will set a regular time each week, multiple times a week as needed, to work with the group on the project. I will also communicate with my group in a clear and timely manner. I will write the sections assigned to me for the project report and present the portion assigned to me for the final presentation.\nThe topics I would like to explore in a project would be making predictions, classifying music or items, or a recognition projection (such as music or emotions or an image recognition)."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Daniela Delgado\nMachine Learning: Course Blog Posts\nMiddlebury College ’24"
  },
  {
    "objectID": "WarmUps/WarmUp4.html",
    "href": "WarmUps/WarmUp4.html",
    "title": "Daniela's CSCI 0451 Blog",
    "section": "",
    "text": "When was a time in your life in which you felt that you were the subject of an unfair decision? What was it about that decision that made it feel unfair, rather than just bad, disappointing, or surprising?\n- A decision that was made unfairly was that when I was bout 6 or 7, my family was going to watch the Astros play in the stadium, but then my sister and I got into an argument. We were fighting and calling each other names, but in my defense, she started it. As a consequence, my parents cancelled the plans they made. It was unfair because I also got in trouble for something I was only defending myself in and it was disappointing because we loved going to baseball games and that was always a fun time for everyone. I also was sad I missed out on getting an ice cream hat. \nConsider Figure 4 in the Introduction of BHN. In this figure, there are blue dots and green dots, where the colors correspond to hypothetical demographic attributes. We as the reader can choose what the colors mean.\nSuggest one possible meaning for the blue and green dots in which you would say that the classifier depicted in the figure is unproblematic from the perspective of fairness. - Previous experience or not Suggest one possible meaning for the blue and green dots in which you would say that the classifier depicted in the figure is concerning from the perspective of fairness. - Sex: Male or female - This is concerning because there would be an unequal amount of sexes being accepted into colleges. It is also unfair to compare based on sex. What is the relevant difference between the two cases? - The difference between both is that one looks at something that would not apply to the job and you can’t control, while with having experience or not, that applies to the job and it is something you can improve."
  },
  {
    "objectID": "WarmUps/WarmUp1.html",
    "href": "WarmUps/WarmUp1.html",
    "title": "Daniela's CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\n#show table\ndf\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nPAL0910\n120\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A2\nNo\n12/1/09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nPAL0910\n121\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A1\nYes\n11/22/09\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n8.41151\n-26.13832\nNaN\n\n\n341\nPAL0910\n122\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A2\nYes\n11/22/09\n50.4\n15.7\n222.0\n5750.0\nMALE\n8.30166\n-26.04117\nNaN\n\n\n342\nPAL0910\n123\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A1\nYes\n11/22/09\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n8.24246\n-26.11969\nNaN\n\n\n343\nPAL0910\n124\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A2\nYes\n11/22/09\n49.9\n16.1\n213.0\n5400.0\nMALE\n8.36390\n-26.15531\nNaN\n\n\n\n\n344 rows × 17 columns\n\n\n\n\n#Warm-Up part A\n\n#group dataset based on sex and species to find mean mass\n\nmeanMassTable = df.groupby(['Species', 'Sex']).aggregate({'Body Mass (g)' : 'mean'}) #using dictionary to apply the mean function on the data\nprint(meanMassTable)\n\n                                                  Body Mass (g)\nSpecies                                   Sex                  \nAdelie Penguin (Pygoscelis adeliae)       FEMALE    3368.835616\n                                          MALE      4043.493151\nChinstrap penguin (Pygoscelis antarctica) FEMALE    3527.205882\n                                          MALE      3938.970588\nGentoo penguin (Pygoscelis papua)         .         4875.000000\n                                          FEMALE    4679.741379\n                                          MALE      5484.836066\n\n\n\n#Warm-up part B\n\n#Scatterplot of culmen length against culmen depth (bill dimensions), with the color of each point corresponding to the penguin species\n\n#creating scatterplot \nsns.scatterplot(data = df, x = 'Culmen Length (mm)', y = 'Culmen Depth (mm)', hue = 'Species')\n\n#labeling plot\nplt.title('Culmen Length vs. Culmen Depth Based on Penguin Species')\nplt.show()"
  }
]